     ПРАКТИЧЕСКОЕ ЗАДАНИЕ ПО КУРСУ
    "ПАРАЛЛЕЛЬНЫЕ МЕТОДЫ СУПЕРКОМПЬЮТЕРНЫХ ВЫЧИСЛЕНИЙ"
     преподаватель: Коньшин Игорь Николаевич

Последнюю версию этого файла можно найти в директории "~student/22-mvm"
на кластере ИВМ РАН или по адресу:
  http://cluster2.inm.ras.ru/~student/22-mvm/practica-mvm.txt
Полный список файлов в директории":
  practica-mvm.txt - этот файл (в кодировке cp1251)
  Notes_*.txt - краткие памятки по кластеру, MPI и OpenMP (в кодировке cp1251)
  mpi_*.c omp_*.c mix_*.c - образцы файлов с распараллеливанием на MPI, OpenMP и MPI+OpenMP для операций daxpy и norm
  qs_mpi qs_omp qs_mix - скрипты для постановки задания в очередь для MPI, OpenMP и MPI+OpenMP
  mvm.plot - скрипт для рисования графиков ускорения
  mvm-*-*.dat - упрощенный образец файлов с результатами экспериментов
  mvm.png - упрощенный образец графиков ускорения
  mvm.c - упрощенный вариант последовательной(!) программы по умножению плотной матрицы на вектор, в котором отмечены места, где нужно дописать небольшой код на OpenMP и MPI
Описание самого кластера можно найти здесь:
  http://cluster2.inm.ras.ru/
хотя вся необходимая информация имеется прямо в этом файле с описанием задания.

--------------------------
Общая формулировка задания
--------------------------
1) распараллелить на MPI алгоритм умножения плотной матрицы на вектор;
2) добавить прямо в эту программу распараллеливание на OpenMP;
3) проверить корректность нормы полученного результата и его независимость
   от количества используемых процессов (нитей);
4) провести численные эксперименты по распараллеливанию на MPI и OpenMP
   и рассчитать реальное ускорение;
5) теоретически оценить ускорение;
6) построить графики ускорения и сравнить теорию с практикой;
7) дополнительно исследовать гибридный параллелизм MPI+OpenMP;
8) написать несколько слов с выводами вашего исследования.

---------------------------
1) Распараллеливание на MPI

При выполнении умножения матрицы на вектор на MPI считается,
что матрица "А" равномерно распределена по процессорам
по своим блочным строкам, причем оба вектора "x" и "y"
распределены по процессорам аналогично:

   y         A        x
  [=]   [== == ==]   [=]
  ---   ----------   ---
  [=] = [== == ==] * [=]
  ---   ----------   ---
  [=]   [== == ==]   [=]

Перед выполнением умножения каждый MPI-процесс хранит только
свои локальные части матрицы "А" и векторов "x" и "y".
Сначала необходимо на каждом из MPI-процессов собрать
полный вектор "X" с помощью обменов данными через MPI
(эта операция должна войти в замер времени).
Затем производится собственно умножение без всяких обменов.
Локальный вектор результата "y" после выполнения умножения остается на месте
и на один процесс не собирается.
Блоксхема программы для MPI:

  ...заполнение своей части матрицы и вектора "x" на каждом из процессоров
  MPI_Barrier(MPI_COMM_WORLD);
  double T = MPI_Wtime();
  ...сборка полного вектора "X"
  ...умножение локальной части матрицы на собранный "X"
  MPI_Barrier(MPI_COMM_WORLD);
  T = MPI_Wtime() - T;
  ...подсчет нормы вектора-результата "y"
  ...печать количества процессов, времени решения "T" и нормы "y"

Исходные данные:
Aij = 1./(1.+i+j), где i и j глобальные номера строк и столбцов, соответственно.
Xi = i,  здесь i,j = 0,...,N-1 в С-шных соглашениях.

Обратите внимание на то, время выполнение каких именно частей алгоритма замеряется, а какие нас интересуют гораздо меньше.

Для написания, компиляции и запуска программы на MPI
смотрите руководства и примеры в файлах:
  Notes_mpi.txt mpi_*.c qs_mpi

------------------------------
2) Распараллеливание на OpenMP

Для распараллеливания программы на OpenMP потребуется
перед внешним циклом добавить директиву препроцессора:
  #pragma omp parallel for

Если понадобится прямо из программы определить количество
используемых нитей OpenMP, можно воспользоваться функцией
omp_get_max_threads(), предварительно включив ее описание посредством
#include <omp.h>

Для написания, компиляции и запуска программы на OpenMP
смотрите руководства и примеры в файлах:
  Notes_omp.txt omp_*.c qs_omp

-----------------------------------------
3) Проверка корректности нормы результата

После получения вектора-результата "y" и окончания замера времени
производится подсчет 2-й нормы вектора-результата "y"
с использованием функции MPI_Allreduce() и 
"#pragma omp parallel for" вместе c "reduction(+:sum)".
Если матрица и вектор "x" заданы как в п.1,
то норма "y" при этом для N=1152 будет равна 20580.1167 и она не должна
изменяться от количества используемых процессов MPI и нитей OpenMP.

-----------------------------------------
4) Численные эксперименты на MPI и OpenMP

Замерить время счета и вычислить реальное ускорение для:
a) p процессов MPI (и одну нить OpenMP),
b) p нитей OpenMP (и процесс MPI),
где p=1,2,3,4,6,8,9,12,16,18,24,32,36.
Вычисления провести в разделе normal (40 вычислительных ядер на узел).
Расчеты провести для размерностей матрицы N=2^7*3^2=1152 и N=2^10*3^2=9216,
где N без остатка делится на приведенные значения p.

В принципе, количество процессов MPI можно было бы взять и побольше,
например, до 8*24=192 процессов, но это совсем не обязательно,
чтобы не огорчаться из-за недостаточно высокого ускорения.

Компиляция и сборка вашей программы prog.c одновременно для MPI и OpenMP:
  mpicc  -fopenmp -O2 prog.c -lm
  mpiicc -qopenmp -O2 prog.c -lm

Поставить в очередь заданий при распараллеливании через MPI:
  sbatch qs_mpi
где файл qs_mpi содержит:
#!/bin/bash
#SBATCH --job-name=22-FIO
#SBATCH --partition normal
#SBATCH --nodes=1 --ntasks-per-node=40
#SBATCH --time=30:00
#SBATCH --chdir=.
rm -f res_mpi
for p in 1 2 3 4 6 8 10 12 16 20 24 32 36 ; do ## x20core
    mpirun -np $p ./a.out >> res_mpi
done

Поставить в очередь заданий при распараллеливании через OpenMP:
  sbatch qs_omp
где файл qs_omp содержит:
#!/bin/bash
#SBATCH --job-name=22-FIO
#SBATCH --partition x12core
#SBATCH --nodes=1 --ntasks-per-node=24
#SBATCH --time=30:00
#SBATCH --chdir=.
rm -f res_omp
for p in 1 2 3 4 6 8 10 12 16 20 24 32 36 ; do ## x20core
    export OMP_NUM_THREADS=$p ; ./a.out >> res_omp
done

Результаты расчетов будут в файлах res_mpi и res_omp, соответственно.
Для рисования графиков их лучше переименовать в 
"mvm-1152-mpi.dat", "mvm-9216-mpi.dat" и "mvm-1152-omp.dat", "mvm-9216-omp.dat",
соответственно, в зависимости от размерности задачи.

---------------------------------
5) Теоретическая оценка ускорения

Вывод формулы для расчета ускорения при MPI распараллеливании алгоритма
на компьютере с распределенной памятью:

p - количество используемых процессов MPI
T(p) - время решения задачи на p процессорах
S - ускорение, S=T(1)/T(p)
E - эффективность, E=S/p

La - общее количество арифметических операций алгоритма
Lc - общая длина всех обменов данными для алгоритма
ta - среднее время выполнения одной арифметической операции
tc - среднее время выполнения обмена одним числом

Ta = La ta     - время затраченное на арифметику (вычисления)
Tc = Lc tc     - время затраченное на коммуникации (обмены)
tau = tc / ta  - общая характеристика параллельного компьютера
L = La / Lc    - общая характеристика параллельности алгоритма

S = S(p) = T(1) / T(p) = Ta / (Ta/p + Tc/p) =

= p Ta / (Ta + Tc) = p / (1 + Tc/Ta) =

= p / (1 + (Lc tc) / (La ta)) =

= p / (1 + tau * L)

Итого:
S(L,p,tau) = p / (1 + tau * L),
т.е. ускорение зависит от характеристик алгоритма (L), компьютера (tau)
и собственно количества процессов (p).

Для оценки ускорения алгоритма умножения матрицы на вектор (см. п.1)
необходимо вычислить La и Lc, найти L=Lc/La
и подставить параметры в формулу S(L,p,tau).
Арифметические затраты (сложения=умножения) на проц.: La = N*N / p,
Комминикационные затраты по сбору  на проц. (посылка=прием): Lc = (N/p)*(p-1).
Итого, L = Lc/La = (p-1) / N
и S = p / (1 + tau * L) = p / (1 + tau * (p-1) / N).
Заметим, что L(p=1)=0 и S(p=1)=1 как и требуется.
С ростом tau (или уменьшением обменов Lc) ускорение S растет.
Если подсчитать tau для x20core, то получим примерно tau=30.
Это значение можно использовать для теоретической оценки ускорения.

---------------------------------------
6) Графики ускорения: теория и практика

Для построения графиков можно использовать любые удобные вам средства:
excel, python, gnuplot, и др. (по возможности не рисовать от руки).
Ниже приведен пример использования gnuplot непосредственно на кластере.

Следует провести расчеты, например, для размерностей матрицы N=1152 и N=9216,
на доступном количестве процессоров, например, p=1,2,3,4,6,...,72.
Результаты расчетов записать в файлы "mvm-1152.dat" и "mvm-9216.dat",
соответственно.
Ускорение S можно подсчитать с помощью калькулятора, или команды "bc",
или же воспользоваться готовым скриптом прямо на кластере:

$ ~students/bin/x_bc_ptTS mvm-XXXX-YYY.dat

где "mvm-XXXX-YYY.dat" - это файл с результатами
(XXXX - размерность задачи N, YYY - суффикс "mpi" или "omp"), где:
1-ая колонка - значение "p" (processes, процессы MPI),
2-ая колонка - значение "t" (threads, нити OpenMP),
3-я  - замеренное время умножения
На экран будет выдана таблица, где в 4-ой колонке появится
автоматически подсчитанное ускорение.

В директории имеется файл "mvm.plot", содержащий:
set terminal png enhanced large size 512, 512
set output "mvm.png"
set title "Speedup of parallel MVM"
set key left
set xlabel "p"
set ylabel "S"
y(x) = x ### linear speedup
### Here: S(p)=p/(1+tau*L), where tau=tc/ta and L=Lc/La
tau=30. ; La(p,N)=N*N/p ; Lc(p,N)=N/p*(p-1) ; L(p,N)=Lc(p,N)/La(p,N) ### estimates for MVM
S(p,N) = p/(1.+tau*L(p,N)) ### speedup estimation
plot y(x)      with lines lw 1 lc 0 title "linear speedup" ,\
     S(x,1152) with lines lw 1 lc 1 title "theory N=1152 mpi" ,\
     S(x,9216) with lines lw 2 lc 1 title "theory N=9216 mpi" ,\
     "mvm-1152-mpi.dat" using 1:4 with linespoints lw 2 lc 3 pt 2 title "mpi N=1152" ,\
     "mvm-9216-mpi.dat" using 1:4 with linespoints lw 3 lc 3 pt 6 title "mpi N=9216" ,\
     "mvm-1152-omp.dat" using 2:4 with linespoints lw 2 lc 2 pt 2 title "omp N=1152" ,\
     "mvm-9216-omp.dat" using 2:4 with linespoints lw 3 lc 2 pt 6 title "omp N=9216"

В директории имеются упрощенные образцы всех необходимых файлов
с данными экспериментов "mvm-*-*.dat".
Графики ускорения для своих файлов с данными можно нарисовать
с помощью команды "gnuplot":
  gnuplot mvm.plot

Появится ваш собственный файл "mvm.png" с графиками ускорения.
Для просмотра графика прямо на кластере можно скопировать его
в директорию ~/public_html и смотреть его прямо из браузера по ссылке
  http://cluster2.inm.ras.ru/~student/mvm.png
а потом сохранить его на свой компьютер и прикрепить к отчету.

В приведенном примере на одном графике наложены результаты по MPI и OpenMP,
а также теоретическое (см п.5) и практически полученное ускорение.

Т.е. на одном графике лучше разместить сразу все 7 кривых:
- линейное ускорение S=p (как образец "идеального" ускорения)
- теоретические ускорения для N=1152 и N=9216
- практические ускорения на MPI для N=1152 и N=9216
- практические ускорения на OpenMP для N=1152 и N=9216

К отрисовке графика ускорения относятся следующие файлы:
  mvm.plot mvm-*-*.dat mvm.png

-----------------------------------------
7) Гибридное распараллеливание MPI+OpenMP

Можно провести такой эксперимент на большой размерности N=9216.
Поставить в очередь заданий при гибридном распараллеливании через MPI+OpenMP:
  sbatch qs_mix
где файл qs_mix содержит:
#!/bin/bash
#SBATCH --job-name=22-FIO
#SBATCH --partition normal
#SBATCH --nodes=1 --ntasks-per-node=36
#SBATCH --time=30:00
#SBATCH --chdir=.
rm -f res_mix
## Here: p - number of MPI processes ; t - number of OMP threads ; pt=p*t
pt=$SLURM_NTASKS
 for p in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ; do
     t=$(($pt / $p))
     if [[ $pt == $(($p * $t)) ]] ; then
         export OMP_NUM_THREADS=$t ; mpirun -np $p ./a.out >> res_mix
     fi
done

Составить таблицу с колонками "p" (процессы MPI), "t" (нити OpenMP) и "Время=T"
(при этом p*t=36, т.е. суммарное количество ядер здесь одинаково!)
и определить при какой комбинации "p" и "t" время будет минимальным.
Прислать саму таблицу или график зависимости времени умножения "T"
от "p" кол-ва процессов MPI.

К эксперименту по гибридному MPI+OpenMP распараллеливанию относятся файлы:
  mix_*.c qs_mix

----------------
8) Краткий отчет

Прислать полученные графики, а также
написать несколько слов с выводами вашего исследования:
(a) Растет или уменьшается ускорение с ростом размерности задачи "N"?
(b) Какое распараллеливание работает быстрее: MPI или OpenMP?
(c) Есть ли на экспериментальных кривых замедление при больших "p" или "t"?
(d) Почему практическое ускорение отличается от теоретического?
(e) Каково оптимальное соотношение "p" и "t" при гибридном распараллеливании?

